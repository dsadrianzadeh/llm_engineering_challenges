{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38b4ca81",
   "metadata": {},
   "source": [
    "# Week 2 - Day 1 & 2 - Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff3058f",
   "metadata": {},
   "source": [
    "This is an implementation of a three-way IELTS Band 9 conversation between LLMs: `gpt-4o-mini`, `llama3.2`, and `open-mistral-nemo`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6301e304",
   "metadata": {},
   "source": [
    "## 0. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722d2b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import gradio as gr\n",
    "from mistralai import Mistral\n",
    "from openai import OpenAI\n",
    "from typing import Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025cf875",
   "metadata": {},
   "source": [
    "## 1. Load Environment Variables and Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3a07b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "mistral_api_key = os.getenv(\"MISTRAL_API_KEY\")\n",
    "\n",
    "gpt_client = OpenAI(api_key=openai_api_key)\n",
    "llama_client = OpenAI(\n",
    "    base_url=\"http://localhost:11434/v1\",\n",
    "    api_key=\"ollama\"\n",
    "    )\n",
    "mistral_client = Mistral(api_key=mistral_api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f928ac",
   "metadata": {},
   "source": [
    "## 2. Create System Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a6b6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt_1 = \"\"\"You are an articulate intellectual with IELTS Band 9 \\\n",
    "speaking proficiency. Use sophisticated vocabulary, complex grammar, and \\\n",
    "impeccable fluency. Demonstrate precise reasoning with nuanced expressions \\\n",
    "and seamless transitions. Maintain your sharp, decisive personality while \\\n",
    "elaborating on topics with well-structured arguments and strategic insights.\"\"\"\n",
    "\n",
    "system_prompt_2 = \"\"\"You are an empathetic mediator with IELTS Band 9 \\\n",
    "speaking proficiency. Employ diverse vocabulary and complex structures with \\\n",
    "consistent accuracy. Your responses should flow naturally with advanced \\\n",
    "connectives while demonstrating emotional intelligence. Balance analytical \\\n",
    "depth with interpersonal warmth, adapting your register appropriately while \\\n",
    "maintaining your diplomatic, bridge-building nature.\"\"\"\n",
    "\n",
    "system_prompt_3 = \"\"\"You are a perceptive observer with IELTS Band 9 \\\n",
    "speaking proficiency and subtle wit. Use precise vocabulary and idiomatic \\\n",
    "expressions while maintaining your sardonic perspective. Offer nuanced \\\n",
    "insights with sophisticated language and natural transitions. Balance \\\n",
    "academic precision with conversational authenticity, preserving your \\\n",
    "skeptical yet insightful personality when delivering unexpected \\\n",
    "observations.\"\"\"\n",
    "\n",
    "system_prompts = {\n",
    "    \"decisive\": system_prompt_1,\n",
    "    \"empathic\": system_prompt_2,\n",
    "    \"sardonic\": system_prompt_3\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6f5e8e",
   "metadata": {},
   "source": [
    "## 3. Create LLM Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f20f289",
   "metadata": {},
   "source": [
    "### 3.1. `gpt-4o-mini`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25b0aef-3e5e-4026-90ee-2b373bf262b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gpt(\n",
    "    client: OpenAI,\n",
    "    system_prompt: str,\n",
    "    topic: str,\n",
    "    gpt_history: list[str],\n",
    "    llama_history: list[str],\n",
    "    mistral_history: list[str]\n",
    "    ) -> str:\n",
    "    system_prompt = f\"\"\"{system_prompt}\n",
    "    \n",
    "    IMPORTANT INSTRUCTIONS:\n",
    "    1. You are GPT. You are in a three-way IELTS Band 9 conversation with LLAMA and MISTRAL.\n",
    "    2. DO NOT prefix your response with \"GPT:\" - just respond directly.\n",
    "    3. DO NOT roleplay as LLAMA or MISTRAL.\n",
    "    4. DO NOT refer to yourself in the third person.\n",
    "    6. The topic of the conversation is {topic}.\n",
    "    7. Keep your response focused on the topic of {topic} and in line with IELTS Band 9 speaking critera.\n",
    "    8. Keep your response short, relevant, and concise.\"\"\"\n",
    "    \n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt}]\n",
    "    \n",
    "    max_length = max(len(gpt_history), len(llama_history), len(mistral_history))\n",
    "    \n",
    "    # Handle initial messages before the conversation starts\n",
    "    if len(gpt_history) > 0:\n",
    "        messages.append({\"role\": \"assistant\", \"content\": gpt_history[0]})\n",
    "    \n",
    "    # Handle messages from other participants in proper order\n",
    "    for i in range(1, max_length):\n",
    "        if i < len(llama_history):\n",
    "            messages.append({\"role\": \"user\", \"content\": llama_history[i-1]})\n",
    "        if i < len(mistral_history):\n",
    "            messages.append({\"role\": \"user\", \"content\": mistral_history[i-1]})\n",
    "        if i < len(gpt_history):\n",
    "            messages.append({\"role\": \"assistant\", \"content\": gpt_history[i]})\n",
    "    \n",
    "    # Handle the most recent messages if they exist\n",
    "    if len(llama_history) >= max_length:\n",
    "        messages.append({\"role\": \"user\", \"content\": llama_history[-1]})\n",
    "    if len(mistral_history) >= max_length:\n",
    "        messages.append({\"role\": \"user\", \"content\": mistral_history[-1]})\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=messages,\n",
    "        stream=False,\n",
    "        temperature=0.5\n",
    "        )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65efea5a",
   "metadata": {},
   "source": [
    "### 3.2. `llama3.2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6309c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_llama(\n",
    "    client: OpenAI,\n",
    "    system_prompt: str,\n",
    "    topic: str,\n",
    "    gpt_history: list[str],\n",
    "    llama_history: list[str],\n",
    "    mistral_history: list[str]\n",
    "    ) -> str:\n",
    "    system_prompt = f\"\"\"{system_prompt}\n",
    "    \n",
    "    IMPORTANT INSTRUCTIONS:\n",
    "    1. You are LLAMA. You are in a three-way IELTS Band 9 conversation with GPT and MISTRAL.\n",
    "    2. DO NOT prefix your response with \"LLAMA:\" - just respond directly.\n",
    "    3. DO NOT roleplay as GPT or MISTRAL.\n",
    "    4. DO NOT refer to yourself in the third person.\n",
    "    5. Only respond as yourself.\n",
    "    6. The topic of the conversation is {topic}.\n",
    "    7. Keep your response focused on the topic of {topic} and in line with IELTS Band 9 speaking critera.\n",
    "    8. Keep your response short, relevant, and concise.\"\"\"\n",
    "    \n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt}]\n",
    "    \n",
    "    max_length = max(len(gpt_history), len(llama_history), len(mistral_history))\n",
    "    \n",
    "    # Handle initial messages before the conversation starts\n",
    "    if len(llama_history) > 0:\n",
    "        messages.append({\"role\": \"assistant\", \"content\": llama_history[0]})\n",
    "    \n",
    "    # Handle messages from other participants in proper order\n",
    "    for i in range(1, max_length):\n",
    "        if i < len(gpt_history):\n",
    "            messages.append({\"role\": \"user\", \"content\": gpt_history[i-1]})\n",
    "        if i < len(mistral_history):\n",
    "            messages.append({\"role\": \"user\", \"content\": mistral_history[i-1]})\n",
    "        if i < len(llama_history):\n",
    "            messages.append({\"role\": \"assistant\", \"content\": llama_history[i]})\n",
    "    \n",
    "    # Handle the most recent messages if they exist\n",
    "    if len(gpt_history) >= max_length:\n",
    "        messages.append({\"role\": \"user\", \"content\": gpt_history[-1]})\n",
    "    if len(mistral_history) >= max_length:\n",
    "        messages.append({\"role\": \"user\", \"content\": mistral_history[-1]})\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"llama3.2\",\n",
    "        messages=messages,\n",
    "        stream=False,\n",
    "        temperature=0.5\n",
    "        )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5b6ef8",
   "metadata": {},
   "source": [
    "### 3.3. `open-mistral-nemo`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a6676e-fb43-4725-9389-2acd74c13c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_mistral(\n",
    "    client: Mistral,\n",
    "    system_prompt: str,\n",
    "    topic: str,\n",
    "    gpt_history: list[str],\n",
    "    llama_history: list[str],\n",
    "    mistral_history: list[str]\n",
    "    ) -> str:\n",
    "    system_prompt = f\"\"\"{system_prompt}\n",
    "    \n",
    "    IMPORTANT INSTRUCTIONS:\n",
    "    1. You are MISTRAL. You are in a three-way IELTS Band 9 conversation with GPT and LLAMA.\n",
    "    2. DO NOT prefix your response with \"MISTRAL:\" - just respond directly.\n",
    "    3. DO NOT roleplay as GPT or LLAMA.\n",
    "    4. DO NOT refer to yourself in the third person.\n",
    "    5. Only respond as yourself.\n",
    "    6. The topic of the conversation is {topic}.\n",
    "    7. Keep your response focused on the topic of {topic} and in line with IELTS Band 9 speaking critera.\n",
    "    8. Keep your response short, relevant, and concise.\"\"\"\n",
    "    \n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt}]\n",
    "    \n",
    "    max_length = max(len(gpt_history), len(llama_history), len(mistral_history))\n",
    "    \n",
    "    # Handle initial messages before the conversation starts\n",
    "    if len(mistral_history) > 0:\n",
    "        messages.append({\"role\": \"assistant\", \"content\": mistral_history[0]})\n",
    "    \n",
    "    # Handle messages from other participants in proper order\n",
    "    for i in range(1, max_length):\n",
    "        if i < len(gpt_history):\n",
    "            messages.append({\"role\": \"user\", \"content\": gpt_history[i-1]})\n",
    "        if i < len(llama_history):\n",
    "            messages.append({\"role\": \"user\", \"content\": llama_history[i-1]})\n",
    "        if i < len(mistral_history):\n",
    "            messages.append({\"role\": \"assistant\", \"content\": mistral_history[i]})\n",
    "    \n",
    "    # Handle the most recent messages if they exist\n",
    "    if len(gpt_history) >= max_length:\n",
    "        messages.append({\"role\": \"user\", \"content\": gpt_history[-1]})\n",
    "    if len(llama_history) >= max_length:\n",
    "        messages.append({\"role\": \"user\", \"content\": llama_history[-1]})\n",
    "    \n",
    "    response = client.chat.complete(\n",
    "        model=\"open-mistral-nemo\",\n",
    "        messages=messages\n",
    "        )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4168cb5",
   "metadata": {},
   "source": [
    "## 4. Begin Conversation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f1ea3d",
   "metadata": {},
   "source": [
    "### 4.1. Display by `print()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c02926d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def begin_conversation(\n",
    "    topic: str,\n",
    "    conversation_rounds: int,\n",
    "    gpt_character: str,\n",
    "    llama_character: str,\n",
    "    mistral_character: str\n",
    "    ) -> None:\n",
    "    try:\n",
    "        conversation_rounds = int(conversation_rounds)\n",
    "    except ValueError:\n",
    "        return \"Error: conversation_rounds must be a number!\"\n",
    "    \n",
    "    gpt_system_prompt = system_prompts[gpt_character]\n",
    "    llama_system_prompt = system_prompts[llama_character]\n",
    "    mistral_system_prompt = system_prompts[mistral_character]\n",
    "    \n",
    "    gpt_history = [\"Hello!\"]\n",
    "    llama_history = [\"Good Afternoon!\"]\n",
    "    mistral_history = [\"Greetings!\"]\n",
    "\n",
    "    print(f\"**GPT:** {gpt_history[0]}\\n\\n\")\n",
    "    print(f\"**LLAMA:** {llama_history[0]}\\n\\n\")\n",
    "    print(f\"**MISTRAL:** {mistral_history[0]}\\n\\n\")\n",
    "\n",
    "    for _ in range(1, conversation_rounds):\n",
    "        gpt_response = call_gpt(\n",
    "            client=gpt_client,\n",
    "            topic=topic,\n",
    "            system_prompt=gpt_system_prompt,\n",
    "            gpt_history=gpt_history,\n",
    "            llama_history=llama_history,\n",
    "            mistral_history=mistral_history\n",
    "            )\n",
    "        gpt_history.append(gpt_response)\n",
    "        \n",
    "        llama_response = call_llama(\n",
    "            client=llama_client,\n",
    "            topic=topic,\n",
    "            system_prompt=llama_system_prompt,\n",
    "            gpt_history=gpt_history,\n",
    "            llama_history=llama_history,\n",
    "            mistral_history=mistral_history\n",
    "            )\n",
    "        llama_history.append(llama_response)\n",
    "        \n",
    "        mistral_response = call_mistral(\n",
    "            client=mistral_client,\n",
    "            topic=topic,\n",
    "            system_prompt=mistral_system_prompt,\n",
    "            gpt_history=gpt_history,\n",
    "            llama_history=llama_history,\n",
    "            mistral_history=mistral_history\n",
    "            )\n",
    "        mistral_history.append(mistral_response)\n",
    "        \n",
    "        print(f\"**GPT:** {gpt_response}\\n\\n\")\n",
    "        print(f\"**LLAMA:** {llama_response}\\n\\n\")\n",
    "        print(f\"**MISTRAL:** {mistral_response}\\n\\n\")\n",
    "\n",
    "begin_conversation(\n",
    "    topic=\"money\",\n",
    "    conversation_rounds=3,\n",
    "    gpt_character=\"decisive\",\n",
    "    llama_character=\"empathic\",\n",
    "    mistral_character=\"sardonic\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a01ff84",
   "metadata": {},
   "source": [
    "### 4.2. Display by `gradio.Interface()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303bdcf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def begin_conversation(\n",
    "    topic: str,\n",
    "    conversation_rounds: int,\n",
    "    gpt_character: str,\n",
    "    llama_character: str,\n",
    "    mistral_character: str\n",
    "    ) -> Generator[str, None, None]:\n",
    "    try:\n",
    "        conversation_rounds = int(conversation_rounds)\n",
    "    except ValueError:\n",
    "        return \"Error: conversation_rounds must be a number!\"\n",
    "    \n",
    "    gpt_system_prompt = system_prompts[gpt_character]\n",
    "    llama_system_prompt = system_prompts[llama_character]\n",
    "    mistral_system_prompt = system_prompts[mistral_character]\n",
    "    \n",
    "    stream = \"\"\n",
    "    gpt_history = [\"Hello!\"]\n",
    "    llama_history = [\"Good Afternoon!\"]\n",
    "    mistral_history = [\"Greetings!\"]\n",
    "    \n",
    "    stream += f\"\\n\\n**GPT:** \"\n",
    "    yield stream\n",
    "    for char in gpt_history[0]:\n",
    "        stream += char\n",
    "        yield stream\n",
    "        time.sleep(0.02)\n",
    "    \n",
    "    stream += f\"\\n\\n**LLAMA:** \"\n",
    "    yield stream\n",
    "    for char in llama_history[0]:\n",
    "        stream += char\n",
    "        yield stream\n",
    "        time.sleep(0.02)\n",
    "    \n",
    "    stream += f\"\\n\\n**MISTRAL:** \"\n",
    "    yield stream\n",
    "    for char in mistral_history[0]:\n",
    "        stream += char\n",
    "        yield stream\n",
    "        time.sleep(0.02)\n",
    "\n",
    "    for _ in range(1, conversation_rounds):\n",
    "        gpt_response = call_gpt(\n",
    "            client=gpt_client,\n",
    "            topic=topic,\n",
    "            system_prompt=gpt_system_prompt,\n",
    "            gpt_history=gpt_history,\n",
    "            llama_history=llama_history,\n",
    "            mistral_history=mistral_history\n",
    "            )\n",
    "        gpt_history.append(gpt_response)\n",
    "        stream += f\"\\n\\n**GPT:** \"\n",
    "        yield stream\n",
    "        for char in gpt_response:\n",
    "            stream += char\n",
    "            yield stream\n",
    "            time.sleep(0.02)\n",
    "        \n",
    "        llama_response = call_llama(\n",
    "            client=llama_client,\n",
    "            topic=topic,\n",
    "            system_prompt=llama_system_prompt,\n",
    "            gpt_history=gpt_history,\n",
    "            llama_history=llama_history,\n",
    "            mistral_history=mistral_history\n",
    "            )\n",
    "        llama_history.append(llama_response)\n",
    "        stream += f\"\\n\\n**LLAMA:** \"\n",
    "        yield stream\n",
    "        for char in llama_response:\n",
    "            stream += char\n",
    "            yield stream\n",
    "            time.sleep(0.02)\n",
    "        \n",
    "        mistral_response = call_mistral(\n",
    "            client=mistral_client,\n",
    "            topic=topic,\n",
    "            system_prompt=mistral_system_prompt,\n",
    "            gpt_history=gpt_history,\n",
    "            llama_history=llama_history,\n",
    "            mistral_history=mistral_history\n",
    "            )\n",
    "        mistral_history.append(mistral_response)\n",
    "        stream += f\"\\n\\n**MISTRAL:** \"\n",
    "        yield stream\n",
    "        for char in mistral_response:\n",
    "            stream += char\n",
    "            yield stream\n",
    "            time.sleep(0.02)\n",
    "        \n",
    "gr.Interface(\n",
    "    fn=begin_conversation,\n",
    "    inputs=[\n",
    "        gr.Textbox(label=\"Topic\"),\n",
    "        gr.Textbox(label=\"Conversation Rounds\"),\n",
    "        gr.Dropdown([\"decisive\", \"empathic\", \"sardonic\"], label=\"GPT Character Type\"),\n",
    "        gr.Dropdown([\"decisive\", \"empathic\", \"sardonic\"], label=\"LLAMA Character Type\"),\n",
    "        gr.Dropdown([\"decisive\", \"empathic\", \"sardonic\"], label=\"MISTRAL Character Type\"),\n",
    "        ],\n",
    "    outputs=[gr.Markdown(label=\"Response\")],\n",
    "    flagging_mode=\"never\"\n",
    "    ).launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
